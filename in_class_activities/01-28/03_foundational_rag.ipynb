{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Foundational RAG Pipeline\n",
    "\n",
    "**Retrieval-Augmented Generation**\n",
    "\n",
    "Based on: https://github.com/NirDiamant/rag_techniques\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the RAG pipeline and why it matters\n",
    "- Implement document chunking with different strategies\n",
    "- Create embeddings and store them in a vector database\n",
    "- Build a simple retriever to find relevant context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain==1.2.7 langchain-community langchain-groq langchain-huggingface langchain-text-splitters faiss-cpu sentence-transformers python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Groq API key\n",
    "if not os.getenv('GROQ_API_KEY'):\n",
    "    os.environ['GROQ_API_KEY'] = input('Enter your Groq API key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** solves two key problems with LLMs:\n",
    "\n",
    "1. **Knowledge**: LLMs only know what they were trained on\n",
    "2. **Hallucination**: LLMs can make up facts\n",
    "\n",
    "**Solution**: Before generating, retrieve relevant information from a knowledge base and include it in the prompt.\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     INDEXING (one-time)                         ‚îÇ\n",
    "‚îÇ        Document ‚Üí Chunk ‚Üí Embed ‚Üí Store in Vector DB            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     RETRIEVAL (per query)                       ‚îÇ\n",
    "‚îÇ     Query ‚Üí Embed ‚Üí Search Vector DB ‚Üí Get Relevant Chunks      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        GENERATION                               ‚îÇ\n",
    "‚îÇ       Query + Retrieved Context ‚Üí LLM ‚Üí Answer                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Loading\n",
    "\n",
    "First, let's load our sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the CCI undergraduate catalog document\n",
    "loader = TextLoader(\"data/CCI_2022-2023-Undergraduate-Catalog.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Check what we loaded\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Document length: {len(documents[0].page_content)} characters\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunking\n",
    "\n",
    "Documents are often too long to fit in an LLM's context window, and we only need relevant parts anyway. **Chunking** splits documents into smaller pieces.\n",
    "\n",
    "### Key Parameters\n",
    "- **chunk_size**: Maximum characters per chunk\n",
    "- **chunk_overlap**: Characters shared between consecutive chunks (prevents cutting off context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximum characters per chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split at these boundaries first\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from the document\")\n",
    "print(f\"\\n--- Chunk 1 ---\")\n",
    "print(chunks[0].page_content)\n",
    "print(f\"\\n--- Chunk 10 ---\")\n",
    "print(chunks[9].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Different Chunk Sizes\n",
    "\n",
    "Let's see how chunk size affects the number and content of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different chunk sizes\n",
    "for size in [200, 500, 1000]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size, \n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split at these boundaries first\n",
    "    )\n",
    "    \n",
    "    test_chunks = splitter.split_documents(documents)\n",
    "    avg_len = sum(len(c.page_content) for c in test_chunks) / len(test_chunks)\n",
    "\n",
    "    print(f\"Chunk size {size}: {len(test_chunks)} chunks, avg length: {avg_len:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trade-offs**:\n",
    "- **Smaller chunks**: More precise retrieval, but may lose context\n",
    "- **Larger chunks**: More context, but may include irrelevant information\n",
    "\n",
    "A common starting point is **500-1000 characters** with **10-20% overlap**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embeddings\n",
    "\n",
    "**Embeddings** convert text into numerical vectors that capture meaning. Similar texts have similar vectors.\n",
    "\n",
    "- a) \"Machine learning is AI\"  ‚Üí  [0.2, -0.5, 0.8, ...]\n",
    "- b) \"AI and ML are related\"   ‚Üí  [0.3, -0.4, 0.7, ...]  \n",
    "- c) \"I like pizza\"            ‚Üí  [-0.8, 0.1, 0.2, ...]  \n",
    "\n",
    "### Libraries:\n",
    "**sentence-transformers**\n",
    "- Developed by HuggingFace for semantic text embeddings\n",
    "- Provides pre-trained models that can convert text into dense vector representations (embeddings)\n",
    "\n",
    "**langchain-huggingface**\n",
    "- LangChain integration package that wraps sentence-transformers\n",
    "- Provides LangChain-compatible interfaces to use HuggingFace models in LangChain workflows\n",
    "\n",
    "**all-MiniLM-L6-v2 embedding model**\n",
    "- https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embedding model (downloads on first run, ~90MB)\n",
    "print(\"Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Fast and good quality\n",
    "    model_kwargs={'device': 'cpu'}   # Use 'cuda' if you have a GPU\n",
    ")\n",
    "print(\"Embedding model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what embeddings look like\n",
    "test_text = \"Machine learning is a type of artificial intelligence.\"\n",
    "test_embedding = embeddings.embed_query(test_text)\n",
    "\n",
    "# We will only print the first 10 entries out of 384.\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Embedding dimensions: {len(test_embedding)}\")\n",
    "print(f\"First 10 values: {test_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Similarity is Measured: Cosine Similarity\n",
    "\n",
    "**Cosine similarity** measures the angle between two vectors, ranging from -1 to 1:\n",
    "- **1.0**: Identical meaning (0¬∞ angle)\n",
    "- **0.0**: No relationship (90¬∞ angle) \n",
    "- **-1.0**: Opposite meaning (180¬∞ angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate similarity - similar texts have similar embeddings\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    \"Machine learning is a type of AI\",\n",
    "    \"AI and machine learning are closely related\",\n",
    "    \"I like pizza\"\n",
    "]\n",
    "\n",
    "embs = [embeddings.embed_query(t) for t in texts]\n",
    "\n",
    "# Calculate cosine similarity between first text and others\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(\"Similarity to 'Machine learning is a type of AI':\")\n",
    "for i, text in enumerate(texts):\n",
    "    sim = cosine_similarity(embs[0], embs[i])\n",
    "    print(f\"  {sim:.3f} - '{text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vector Store (FAISS)\n",
    "\n",
    "### Vector Store\n",
    "A specialized database optimized for:\n",
    "- **Storing** high-dimensional vectors (embeddings)\n",
    "- **Indexing** vectors for fast retrieval\n",
    "- **Searching** for similar vectors using distance metrics (e.g., cosine similarity)\n",
    "\n",
    "### FAISS\n",
    "- **Free & Open Source**: Developed by Meta AI Research\n",
    "- **Runs Locally**: No API calls, no cloud costs\n",
    "- **Fast**: Optimized for billion-scale similarity searches\n",
    "\n",
    "**Alternative Vector Stores:**\n",
    "- **Pinecone**, **Weaviate**, **Qdrant**: Cloud-hosted (require API keys)\n",
    "- **Chroma**, **LanceDB**: Other local options similar to FAISS\n",
    "\n",
    "**GitHub**: https://github.com/facebookresearch/faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create vector store from our chunks\n",
    "print(f\"Creating vector store from {len(chunks)} chunks...\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Vector store created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building a Retriever\n",
    "\n",
    "A **retriever** wraps the vector store and provides a clean interface for getting relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  \n",
    "    search_kwargs={\"k\": 3}     # Number of results to return\n",
    ")\n",
    "\n",
    "# Use the retriever\n",
    "query = \"What are the graduation requirements for CCI students?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nRetrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(doc.page_content[:300] + \"...\" if len(doc.page_content) > 300 else doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete RAG Pipeline\n",
    "\n",
    "Now let's put it all together: retrieve context and generate an answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\", temperature=0.3)\n",
    "\n",
    "def simple_rag(question: str) -> str:\n",
    "    \"\"\"A simple RAG pipeline: retrieve context, then generate answer.\"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    # Step 2: Create prompt with context\n",
    "    prompt = f\"\"\"Answer the question based ONLY on the following context. \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Test the RAG pipeline\n",
    "question = \"What are the graduation requirements for CCI students?\"\n",
    "answer = simple_rag(question)\n",
    "\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "print(f\"\\nüí¨ Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try more questions!\n",
    "questions = [\n",
    "    \"What courses are required for computer science majors?\",\n",
    "    \"How many credit hours are needed to graduate?\",\n",
    "    \"What degree programs are within the College of Computing and Informatics?\",\n",
    "    \"What is a recipe for chocolate cake?\"  # Not in our document!\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"‚ùì {q}\")\n",
    "    print(f\"üí¨ {simple_rag(q)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned the foundational RAG pipeline:\n",
    "\n",
    "1. **Document Loading**: Load documents from files\n",
    "2. **Chunking**: Split documents into smaller pieces with `RecursiveCharacterTextSplitter`\n",
    "3. **Embeddings**: Convert text to vectors with `HuggingFaceEmbeddings`\n",
    "4. **Vector Store**: Index and search with `FAISS`\n",
    "5. **Retriever**: Clean interface for getting relevant documents\n",
    "6. **Generation**: Combine context with query and send to LLM\n",
    "\n",
    "**Key Parameters to Tune**:\n",
    "- `chunk_size`: 500-1000 is a good starting point\n",
    "- `chunk_overlap`: 10-20% of chunk size\n",
    "- `k`: Number of documents to retrieve (3-5 is common)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
